---
title: 'Data 622 Homework 4: Mental Health Data Modeling'
author: 'Group 4: Dennis Pong, Katie Evers, Richard Zheng, Devin Teran'
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
    code_folding: hide
  pdf_document:
    toc: yes
    toc_depth: '3'
    number_sections: yes
    df_print: kable
    highlight: tango
  html_document:
    df_print: paged
    highlight: pygments
    number_sections: yes
    theme: readable
    toc: yes
    toc_depth: 2
    toc_float: no
    fontsize: 12
editor_options:
  chunk_output_type: inline
fontsize: 11pt
urlcolor: blue
---

```{r setup, cache=FALSE}

library(knitr)
library(tidyverse)
library(caret) # For createDataPartition, featureplot, classification report
library(skimr)    # Used for EDA
#library(corrplot) # For correlation matrix
library(mice) # Multivariate Imputation By Chained Equations
library(micemd)
library(parallel)
library(doParallel)
#library(plyr)
library(VIM)
library(ggplot2)
library(kableExtra)
library(stats)
library(tidymodels)
library(e1071)
library(vcd)

library(ggplot2)
library(Hmisc)
library(Boruta)
## Global options
# ---------------------------------

options(max.print="108")
opts_knit$set(width=31)
```

# **Loading data**

```{r loading-data}
# getwd()
setwd("~/Data 622/repos/data622_fa2021/hw4")
adhd_data <- readxl::read_excel('ADHD_data.xlsx', sheet = "Data", .name_repair = "universal", na = "")

# dim(adhd_data)
str(adhd_data)

# loan_raw <- read.csv('https://raw.githubusercontent.com/metis-macys-66898/data622_fa2021/main/hw3/data/Loan_approval.csv', header = TRUE, na.strings = " ")
# loan_raw[loan_raw==""] <- NA
# loan_raw <- loan_raw %>% mutate_if(is.character, factor)
# loan <- loan_raw
```

## Data Processing Steps

I employed a couple strategies into transforming the data before we can apply models.

-   Imputing only when necessary. Imputing by applying the right method for the right variable type.

-   Make sure to make the fields into factors when they are categorical variables.

```{r}

aggr_plot <- aggr(adhd_data, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(data), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))


```

Removing Initial as it provides no values to any models. Also, I removed Psych.meds. as it has 67% missing.

```{r}

adhd_data_pre_imp <- adhd_data %>% select(-c('Initial','Psych.meds.'))
skim(adhd_data_pre_imp) %>% dplyr::filter( n_missing > 0 ) 

```

```{r}
adhd_data_pre_imp <- adhd_data_pre_imp %>% mutate(NonSubstDx =  as.factor(Non.subst.Dx)  ,
                                                  SubstDx = as.factor(Subst.Dx) ,
                                                  Abuse = factor(Abuse, ordered = TRUE), 
                                                  Alcohol = as.factor(Alcohol)
)


adhd_data_pre_imp <- adhd_data_pre_imp %>%  mutate_if(is.numeric, factor)
skim(adhd_data_pre_imp)
adhd_data_pre_imp1 <- adhd_data_pre_imp %>% mutate(Education = as.numeric(Education)  )
skim(adhd_data_pre_imp1)
# skim(adhd_data_pre_imp)
```

### Imputation

More specifically, I separated the variables with missing values into the following categories:

-   **Categorical Variables with more than 2 levels:** Subst.Dx, NonSubstDx, Opioids, Sedative.hypnotics, Stimulants, Cocaine, THC, Alcohol - polyreg

-   **Categorical Variables with 2 levels:** Disorderly.Conduct, Hx.of.Violence, Court.order - logreg

-   **Discrete variable (ordered) with more than 2 levels:** Abuse - polr

-   **Continuous Variables:** Education **- ~~pmm~~ -\> norm**

```{r mice.par}

init <- mice(adhd_data_pre_imp1, maxit=0) 
meth <- init$method
predM <- init$predictorMatrix
meth[c('Education')] <- 'pmm'
meth[c('Disorderly.Conduct', 'Hx.of.Violence', 'Court.order')] <- 'logreg'
meth[c('Subst.Dx', 'NonSubstDx' , 'Opioids', 'Sedative.hypnotics', 'Stimulants', 'Cocaine', 'THC', 'Alcohol')] <- 'polyreg'
meth[c('Abuse')] = 'polr'
meth[c('Age','Sex','Race','ADHD.Q1','ADHD.Q2','ADHD.Q3','ADHD.Q4','ADHD.Q5','ADHD.Q6','ADHD.Q7','ADHD.Q8','ADHD.Q9','ADHD.Q10','ADHD.Q11','ADHD.Q12','ADHD.Q13','ADHD.Q14','ADHD.Q15','ADHD.Q16','ADHD.Q17','ADHD.Q18','ADHD.Total','MD.Q1a','MD.Q1b','MD.Q1c','MD.Q1d','MD.Q1e','MD.Q1f','MD.Q1g','MD.Q1h','MD.Q1i','MD.Q1j','MD.Q1k','MD.Q1L','MD.Q1m','MD.Q2','MD.Q3','MD.TOTAL','Suicide')] = ''

# adhd_data_imp1 <- mice(adhd_data_pre_imp1, method=meth, predictorMatrix=predM, seed=501)
# adhd_data_imp1 <- parlmice(adhd_data_pre_imp1, method=meth, predictorMatrix=predM, cluster.seed=501, m = 4, n.core = 4, n.imp.core = 100)

# no_cores <- detectCores() - 1
# 
# cl<-makePSOCKcluster(no_cores)
# 
# registerDoParallel(cl)
# 
# start.time<-proc.time()
# 
# adhd_data_imp1 <- mice.par(adhd_data_pre_imp1, method=meth, predictorMatrix=predM, seed=301, m = 5)
# 
# stop.time<-proc.time()
# 
# run.time<-stop.time -start.time
# 
# print(run.time)
# 
# stopCluster(cl)
# user  system elapsed 
#   1.826   4.303 264.775 
```

```{r parlmice}

# abandoned this method as it's not always consistent
# cl<-makePSOCKcluster(no_cores)
# registerDoParallel(cl)
#   
# start.time<-proc.time()
#   
# adhd_data_imp1 <- parlmice(adhd_data_pre_imp1, method=meth, predictorMatrix=predM, cluster.seed=501, m = 4, n.core = 4, n.imp.core = 1)
# 
# stop.time<-proc.time()
# 
# run.time<-stop.time -start.time
# 
# print(run.time)
# 
# stopCluster(cl)
```

#### Examining the imputations

Examining the imputations for Education. As the 50-th percentile before was 12, we picked an impute that is exactly that. Impute \#4 fits the bill.

```{r}
# str(adhd_data_imp1)
# skim(adhd_data_imp1) %>% dplyr::filter( n_missing > 0 )
# 
#Eduation
# adhd_data_imp1$imp$Education
# adhd_data_pre_imp1[140:142,]
# 
# 
# #Age
# adhd_data_imp1$imp$Alcohol

```

```{r decision}
# adhd_data_2 <- complete(adhd_data_imp1, 4) # 2nd argument if not provided is defaulted to 1
```

```{r adhd_data_2}

# skim(adhd_data_2) %>% dplyr::filter( n_missing > 0 )
# # # saving a R data frame

# saveRDS(adhd_data_2, "adhd_data_2.rds")


```

Ended up dropping 29 records as even there are other means of imputing the missing records manually. I just don't think it's trustworthy when advanced algorithms did not end up imputing them. The need to have complete cases, except for the suicide variable for section of clustering and PCA where suicide variable is not the response variable, or variable of interest, is the ultimate reason why I had to drop any records with even a missing field from the dataset.

```{r load.RData}
# loading rds object

# adhd_data_2 <- readRDS("adhd_data_2.rds")

# adhd_data_imp1$imp$Suicide
# 41	NA	NA	NA	NA	NA
# 49	NA	NA	NA	NA	NA
# 53	NA	NA	NA	NA	NA
# 67	NA	NA	NA	NA	NA
# 73	NA	NA	NA	NA	NA
# 106	NA	NA	NA	NA	NA
# 117	NA	NA	NA	NA	NA
# 122	NA	NA	NA	NA	NA
# 129	NA	NA	NA	NA	NA
# 131


# adhd_data_pre_imp1[complete.cases(adhd_data_pre_imp1[ , -49]),]

# saved 3 records
# adhd_data_3 <- adhd_data_2[complete.cases(adhd_data_2[ , -49]),]




```

```{r load.adhd_data_3}


# Saving the records 
# saveRDS(adhd_data_3, "adhd_data_3.rds")
#Loading the records
adhd_data_3 <- readRDS("adhd_data_3.rds")
```

#### Count for each Education / suicide attempt

```{r "Education~Suicide"}

ggplot(adhd_data_3, aes(x = Education, fill = Suicide)) +
  geom_bar(alpha = 0.8) +
  scale_fill_manual(values = c("darkorange", "purple", "cyan4"),
                    guide = F) +
  theme_minimal() +
  facet_wrap(~Suicide, ncol = 1) +
  coord_flip()
mosaic(~ Education + Suicide, data = adhd_data_3)
```

#### Count for each Subst.Dx / suicide attempt

```{r "Subst.Dx~Suicide"}

ggplot(adhd_data_3, aes(x = Subst.Dx, fill = Suicide)) +
  geom_bar(alpha = 0.8) +
  scale_fill_manual(values = c("darkorange", "purple", "cyan4"),
                    guide = F) +
  theme_minimal() +
  facet_wrap(~Suicide, ncol = 1) +
  coord_flip()
mosaic(~ Subst.Dx + Suicide, data = adhd_data_3)
```

#### Count for each Non.subst.Dx / suicide attempt

```{r "Non.subst.Dx~Suicide"}

ggplot(adhd_data_3, aes(x = Non.subst.Dx, fill = Suicide)) +
  geom_bar(alpha = 0.8) +
  scale_fill_manual(values = c("darkorange", "purple", "cyan4"),
                    guide = F) +
  theme_minimal() +
  facet_wrap(~Suicide, ncol = 1) +
  coord_flip()
mosaic(~ Non.subst.Dx + Suicide, data = adhd_data_3)
```

#### Count for each Abuse / Suicide

```{r "Abuse~Suicide"}

ggplot(adhd_data_3, aes(x = Abuse, fill = Suicide)) +
  geom_bar(alpha = 0.8) +
  scale_fill_manual(values = c("darkorange", "purple", "cyan4", "black"),
                    guide = F) +
  theme_minimal() +
  facet_wrap(~Suicide, ncol = 1) +
  coord_flip()
mosaic(~ Abuse + Suicide, data = adhd_data_3)
```

## Clustering Method
Here we are going to start our clustering techniques using hierarchical clustering.  The benefits of this clustering type is that we don't need to specify the number of clusters and results are reproducable.
  
Instead of using all the columns from our data, we decided to only use the column *ADHD.Total* an *MD.Total* to the represent the individual columns combined together.  This will help our results to be easier to interpret.  
  
The output from hierarchical clustering is a dendogram, which is a graph that looks like a tree.  We can see from our dendogram below there are 4 distinct clusters.     
```{r Clustering-more-data}
hier_cluster_subset <- adhd_data_3 %>% select(c('Age','Sex','Race','ADHD.Total','MD.TOTAL','Alcohol','THC','Cocaine','Stimulants','Sedative.hypnotics','Opioids','Court.order','Education','Hx.of.Violence','Disorderly.Conduct','Suicide','Abuse','Non.subst.Dx','Subst.Dx','NonSubstDx','SubstDx'))
hier_cluster_complete <- hclust(dist(hier_cluster_subset),method="complete")
plot(hier_cluster_complete, hang = -1, cex = 0.6)
abline(h = 43.5, col = "red")
```
From the above dendogram, we can see there are 4 distinct clusters so we will cut our tree at this level using the function **cutree()**.
This function returns a vector of cluster assignments, which can be see here:  
```{r Clustering-simple-cut-tree}
vect_cut <- cutree(hier_cluster_complete,4)
hier_cluster_subset$cluster_pred <- vect_cut
vect_cut
```
The above clustering included quite a few columns.  Now we are going to make a simpler clustering dendogram using only *age*,*ADHD.Total*,*MD.TOTAL*.  These columns represent our numeric data.  
```{r Clustering-simple}
hier_cluster_subset_simple <- adhd_data_3 %>% select(c('Age','ADHD.Total','MD.TOTAL'))
hier_cluster_complete_simple <- hclust(dist(hier_cluster_subset_simple),method="complete")
plot(hier_cluster_complete_simple, hang = -1, cex = 0.6)
abline(h = 50, col = "red")
```
Based on the above dendogram, we can see there are 3 distinct clusters.  Again, we will cut our tree and view the output cluster assignments.  
```{r Clustering-cut-tree}
num_cut = cutree(hier_cluster_complete_simple,3)
hier_cluster_subset_simple$cluster_pred <- num_cut
num_cut
```
Here we can see that it appears that cluster 2 had a median and average age of 38.  This cluster's age fits right between cluster 1 & 3.  Cluster 2 had the highest individual scores for reporting ADHD and reporting mood disorder.  It would be interesting to see if there is any data to investigate if this age has any correlation with a mid-life crisis.  Younger participants self reported the lower amount of ADHD and mood disorder.  
```{r }
results_simple <- hier_cluster_subset_simple %>% dplyr::group_by(cluster_pred) 
results_simple$Age <- as.numeric(as.character(results_simple$Age))
results_simple$ADHD.Total <- as.numeric(as.character(results_simple$ADHD.Total))
results_simple$MD.TOTAL <- as.numeric(as.character(results_simple$MD.TOTAL))
results_simple %>% dplyr::summarize(Avg_Age = mean(Age,na.rm=TRUE),
                                    Median_Age = median(Age,na.rm=TRUE),
                                    Avg_ADHD.Total = mean(ADHD.Total,na.rm=TRUE),
                                    Median_ADHD.Total = median(ADHD.Total,na.rm=TRUE),
                                    Avg_MD.TOTAL = mean(MD.TOTAL,na.rm=TRUE),
                                    Median_MD.TOTAL = median(MD.TOTAL,na.rm=TRUE))
```


## Principal Component Analysis

```{r PCA}




```

## Xtreme Gradient Boosting (XGBoost)

Removing the lone record that Suicide is NA.

```{r xgboost}
# need to remove Suicide null 
dim(adhd_data_3)
# skim(adhd_data_3) 
adhd_data_4 <- adhd_data_3[complete.cases(adhd_data_3),]
dim(adhd_data_4)
# Dropping the 2 repeated columns
adhd_data_4 <- adhd_data_4[, -c(51:52)]
```

Splitting the train and test dataset

```{r train-test}
set.seed(108)
sample_size <- floor(nrow(adhd_data_4)*0.8)
indices <- sample(1:nrow(adhd_data_4),sample_size)
data_train <- adhd_data_4[c(indices),]
data_test <- adhd_data_4[-c(indices),]
```

One-hot encoding of features, which is a requirement for XGBoost. At the end, you wanted to make sure the number of columns in data\_train and data\_test are the same.

```{r 1-hot}
# do the 1-hot encoding for the data_train
Suicide <- data_train[, 49]
dummy <- dummyVars(" ~ . ", data = data_train [, -49] )
newdata <- data.frame(predict (dummy, newdata = data_train[, -49]))
data_train <- cbind (newdata, Suicide)
options(max.print="309")
colnames(data_train)

# repeat the same exercise for the data_test
Suicide1 <- data_test[, 49]
dummy1 <- dummyVars(" ~ . ", data = data_test [, -49] )
newdata1 <- data.frame(predict (dummy1, newdata = data_test[, -49]))
data_test <- cbind (newdata1, Suicide1)
colnames(data_test)


str(data_train)
```

As you can tell below, the target variable in the data\_train is unbalanced and is skewed toward 0 (68% 0s).

```{r}
table(data_train$Suicide)
```

```{r upsample}
train_upsample <- upSample(x=data_train[, -295], y = data_train$Suicide)
# There is a new variable created called Class so that's why I can drop the Suicide column
table(train_upsample$Class)

```

Preparing the grid for XGBoost

```{r grid_tune}
grid_tune <- expand.grid(
  nrounds = c(81), # of trees normal range is 1500 - 3000 depends on # of records
  max_depth = c(2, 4, 6), 
  eta = c(0.01, 0.1, 0.3), # Learning rate
  gamma = 0,   # pruning normal range [0, 1]
  colsample_bytree = 1, # subsample ratio for columns for tree
  min_child_weight = 1, # the larger the more conservative the model is, can be used as a stop
  subsample = 1 # used to prevent overfitting by sampling x% training dataset
)


```

The following defines the trainControl and actually train the XGBoost

```{r train}
trainControl <- trainControl (method = "cv", 
                              number = 3, 
                              verboseIter = T, 
                              allowParallel = T
                             )

xgb_tune <- train(x = train_upsample[, -295], 
                  y = train_upsample$Class,
                  trControl = trainControl, 
                  tuneGrid = grid_tune, 
                  method = "xgbTree", 
                  verbose = T
                  )

# Best tune
xgb_tune$bestTune

# Creating the best model 
train_Control <- trainControl (method = "none", 
                              verboseIter = T, 
                              allowParallel = T
                             )

final_grid <- expand.grid(nrounds = xgb_tune$bestTune$nrounds, 
                          eta = xgb_tune$bestTune$eta,
                          max_depth = xgb_tune$bestTune$max_depth,
                          gamma = xgb_tune$bestTune$gamma,
                          colsample_bytree = xgb_tune$bestTune$colsample_bytree,
                          min_child_weight = xgb_tune$bestTune$min_child_weight,
                          subsample = xgb_tune$bestTune$subsample
)

xgb_model <- train(x= train_upsample[, -295], 
                   y = train_upsample$Class,
                   trControl = train_Control, 
                   tuneGrid = final_grid, 
                   method = "xgbTree", 
                   verbose = F
)                  


# Creating prediction here
xgb.pred <- predict(xgb_model, data_test)




```

After creating xgb.pred, we run the Confusion Matrix to get the performance metrics.

```{r}
confusionMatrix(xgb.pred, data_test$Suicide)
```

Accuracy is 79.31% while Balanced Accuracy is only 70.24%

## Support Vector Machine (SVM)

SVM will help us decide on optimal decision boundary which can then help classify our labeled data. We're going to model for the response variable suicide attempts with the adhd\_data\_3 dataset with both a radial and a linear kernel to try to find the decision boundary as we don't know whether this is a non-linear problem or linear problem.

```{r}

adhd_data_4_matx = as.matrix(adhd_data_4)

df.corr.p = as.data.frame(rcorr(adhd_data_4_matx)$P)

# removing Suicide, and repeated columns
correlation_table <- cbind(rownames(df.corr.p), df.corr.p[, 49])[-c(49) , ]
correlation_table %>%
  kbl(caption = "Correlation with Suicide") %>%
  kable_material_dark()
```

Here are the factors that has at least a 30% correlation with Suicide:

-   Age

-   ADHD.Q6

-   ADHD.Q9

-   ADHD.Q11

-   ADHD.Q12

-   ADHD.Q13

-   ADHD.Q17

-   ADHD.Q18

-   MD.Q1c

-   MD.Q1e

-   MD.Q1m

-   THC

-   Stimulants

-   Disorderly.Conduct

-   NonSubstDx

We further pear down the list of factors to that of correlation coefficient .500 or above.

```{r ge.5}
svm_selected <- as.data.frame(cbind(rownames(df.corr.p[df.corr.p[, 49] >= .5,]), df.corr.p[df.corr.p[, 49]>= .5, 49 ]))
svm_selected <-  svm_selected[complete.cases(svm_selected), ] 
rownames(svm_selected) <- seq(length=nrow(svm_selected))
svm_selected

adhd <- adhd_data_4 %>% select(c(svm_selected$V1, 'Suicide' )) 
```

```{r radial }
set.seed(45)
indexes = createDataPartition(adhd$Suicide, p = .85, list = F) # results not in a List
train = adhd[indexes, ]
test = adhd[-indexes, ]
 
wts <- 100 / table(test$Suicide)
wts

radial_svm = svm(Suicide~., data=train, class.weights = wts, C = 13)
print(radial_svm)
 
test$pred = predict(radial_svm, test)
 
confusionMatrix(test$pred, test$Suicide)

```

```{r linear}
linear_svm = svm(factor(Suicide)~., data=train, kernel = "linear", type = 'C-classification', cost = 10, scale = F, class.weights = wts)
print(linear_svm)
 
test$pred = predict(linear_svm, test)
 
confusionMatrix(test$pred, test$Suicide)

```

Linear SVM performs better with a higher Balanced Accuracy. Thus, we will focus on improving the linear SVM.

We used correlation coefficients to select the model variables before. Let's take a different approach and examine some charts of the predictor variables and target variable using the ggpairs function. 

```{r ggpairs, warning = FALSE, message = FALSE}
#remove individual ADHD and MD question columns, keeping only the total columns
adhd_data_31<- adhd_data_3[, -c(4:21, 23:37)]
#identify all factor columns
x <- sapply(adhd_data_31, is.factor)
#convert all factor columns to numeric
adhd_data_31[ , x] <- as.data.frame(apply(adhd_data_31[ , x], 2, as.numeric))
adhd_data_31$Suicide <- as.factor(adhd_data_3$Suicide)

ggpairs(adhd_data_31, columns = c(4,5,16,21), aes(colour = Suicide, alpha = 0.4))
```
From the plots above, particularly the box plots, it is evident that MD.TOTAL and SubstDx separate our target variable pretty well. However, note that there is some overlap between the target variable classes. We will use these two variables in our final model.

```{r linear2}
set.seed(125)
adhd_data_model <- subset(adhd_data_3, select=c(SubstDx, MD.TOTAL, Suicide))
indexes = createDataPartition(adhd_data_3_model$Suicide, p = .85, list = F) # results not in a List
train = adhd_data_3_model[indexes, ]
test = adhd_data_3_model[-indexes, ]
  
linear_svm_final = svm(factor(Suicide)~., data=train, kernel = "linear", type = 'C-classification', cost = 10, scale = F, class.weights = wts)
print(linear_svm3)
 
test$pred = predict(linear_svm_final, test3)
 
confusionMatrix(test$pred, test$Suicide)
```
From the confusion matrix results, we can see that 6 cases were misclassified. Accuracy is 71.43% and balanced accuracy is 75.00%.


------------------------------------------------------------------------

```{r}
# set.seed(688)
# # recoding Loan_Status back to categorical variable
# loan_knn2$Loan_Status <- as.factor(loan_knn2$Loan_Status)
# str(loan_knn2)
# 
# 
# # Data Partitioning
# trainIndex <- createDataPartition(loan_knn2$Loan_Status, p = .8, list = FALSE, times = 1)
# knn_train <- loan_knn2[trainIndex,]
# knn_test  <- loan_knn2[-trainIndex,]
# 
# 
# 
# str(knn_train)
```

```{r}
# trControl <- trainControl(method  = "repeatedcv",
#                           repeats  = 11)
# knn.fit <- train(Loan_Status ~ .,
#              method     = "knn",
#              tuneGrid   = expand.grid(k = 1:10),
#              trControl  = trControl,
#              preProcess = c("center","scale"),
#              data       = knn_train
#              )
```

```{r}

# knn_pred <- predict(knn.fit, newdata = knn_test)
# # options('max.print' = 100)  
# # getOption("max.print")
# confusionMatrix(knn_pred, knn_test$Loan_Status)


```
